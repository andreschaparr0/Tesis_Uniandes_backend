\chapter{Estado del Arte}
\label{chap:estado-del-arte}

\section{Sistemas de recomendación para selección de talento}
\subsection{Enfoques clásicos de emparejamiento CV--puesto}

Los enfoques tradicionales de selección de talento se han apoyado fuertemente en los Sistemas de Seguimiento de Candidatos (ATS), los cuales automatizan el filtro inicial de hojas de vida mediante algoritmos de coincidencia de palabras clave (\textit{keyword matching}). Este método consiste fundamentalmente en analizar el texto del currículum en busca de términos específicos extraídos de la descripción del trabajo, tales como habilidades técnicas, títulos de cargo previos o herramientas de software, operando bajo la premisa de que la presencia literal de estos términos indica idoneidad para el puesto \cite{jobswift2024}.

Para refinar este proceso de búsqueda, estos sistemas suelen implementar lógica booleana (uso de operadores AND, OR, NOT) que permite a los reclutadores construir consultas complejas para incluir o excluir perfiles según criterios rígidos. El resultado de este procesamiento es habitualmente una puntuación de relevancia o porcentaje de coincidencia, calculado en función de la frecuencia con la que las palabras clave requeridas aparecen en el documento del candidato, generando así un ranking preliminar que prioriza los perfiles con mayor densidad de términos coincidentes \cite{jobswift2024}.
\subsection{Limitaciones de las soluciones tradicionales}

A pesar de la automatización que ofrecen los sistemas tradicionales, estos enfrentan barreras técnicas significativas relacionadas con el procesamiento de documentos (parsing). Una limitación crítica reside en la incapacidad de muchos ATS antiguos para interpretar correctamente estructuras de diseño complejas. Elementos visuales comunes en hojas de vida modernas, como tablas, cuadros de texto o gráficos, a menudo no son reconocidos por el software de análisis, lo que provoca que la información contenida en ellos sea ignorada o eliminada durante la extracción de datos, resultando en perfiles incompletos \cite{resumemate2025, upskillist2024}.

Un problema técnico frecuente ocurre con el uso de diseños de múltiples columnas. La mayoría de los parsers tradicionales leen el documento de izquierda a derecha y de arriba hacia abajo de manera estrictamente lineal. Esto ocasiona que el texto de una columna se mezcle incorrectamente con el de la columna adyacente, desordenando la información (por ejemplo, mezclando fechas de experiencia con títulos educativos) y haciendo que el contenido sea ininteligible para el algoritmo de emparejamiento \cite{jobscan2024}.

Estas fallas de interpretación obligan a los candidatos a adaptar sus hojas de vida a formatos planos y poco estéticos para asegurar su legibilidad por la máquina. Cuando el sistema no logra parsear el documento correctamente debido a estos errores de formato, el candidato puede ser descartado automáticamente, independientemente de su idoneidad real para el cargo, evidenciando la fragilidad de los métodos de extracción basados en reglas rígidas frente a enfoques más modernos de procesamiento de lenguaje \cite{upskillist2024}.
\section{Extracción y estructuración de CVs y descripciones de trabajo}
\subsection{Pipelines de NLP para procesamiento de documentos}

El desarrollo de un sistema de análisis de hojas de vida (\textit{resume parsing}) implica la construcción de un flujo de procesamiento secuencial que transforma documentos no estructurados en datos organizados. El primer paso fundamental es la conversión del archivo fuente (generalmente PDF o DOCX) a texto plano. En esta etapa se utilizan librerías de extracción específicas (como pdfminer) o tecnologías de OCR cuando el documento es una imagen escaneada, seguidas inmediatamente por una fase de limpieza para eliminar caracteres especiales, espacios redundantes y ruido de formato que pueda interferir con el análisis posterior \cite{geeksforgeeks2024, affinda2025}.

Una vez obtenido el texto limpio, se aplican técnicas de Procesamiento de Lenguaje Natural (NLP) para descomponer y clasificar la información. El proceso inicia con la tokenización y eliminación de palabras vacías (stopwords), preparando el texto para la segmentación. La segmentación divide el documento en bloques lógicos identificables, separando secciones clave como Experiencia Laboral, Educación o Habilidades del resto del contenido, permitiendo que el sistema enfoque su búsqueda en las áreas pertinentes del documento \cite{affinda2025}.

La etapa final y más compleja es la Extracción de Entidades Nombradas (NER, por sus siglas en inglés). Utilizando modelos de lenguaje pre-entrenados (como los disponibles en la librería spaCy), el sistema recorre los segmentos de texto para identificar y etiquetar datos específicos, clasificándolos en categorías como nombres de personas, organizaciones, fechas o competencias técnicas. El resultado de este pipeline es una estructura de datos estandarizada (típicamente en formato JSON o XML) que permite almacenar y comparar la información del candidato de manera programática \cite{geeksforgeeks2024}.
\subsection{Uso de modelos modernos (embeddings y LLMs) para estructuración}

La evolución reciente del Procesamiento de Lenguaje Natural ha introducido paradigmas que superan las limitaciones de las palabras clave. Un avance fundamental es el uso de Embeddings, modelos que transforman oraciones o párrafos enteros en vectores numéricos densos dentro de un espacio multidimensional. A diferencia de los métodos tradicionales, estos vectores capturan el significado semántico del texto, permitiendo que el sistema comprenda que términos distintos (como Machine Learning y Aprendizaje Automático) son equivalentes al estar ubicados cerca en el espacio vectorial, lo cual mejora drásticamente la precisión en tareas de búsqueda y emparejamiento \cite{pinecone2025}.

Paralelamente, la irrupción de los Modelos de Lenguaje Grandes (LLMs), como la serie GPT, ha revolucionado la tarea de extracción de información. A diferencia de los parsers tradicionales que se rompen con diseños complejos, los LLMs poseen una capacidad de comprensión contextual que les permite interpretar hojas de vida con formatos no estándar, tablas o diseños creativos. Estos modelos no se limitan a extraer texto, sino que pueden inferir información implícita y estructurar datos desordenados en formatos rígidos (como JSON) siguiendo instrucciones en lenguaje natural, reduciendo significativamente la necesidad de intervención humana en la limpieza de datos \cite{edenai2024}.

\section{Evaluación y brechas en el estado del arte}
\subsection{Formas de evaluar matching y calidad de extracción}

La validación de un sistema de reclutamiento inteligente requiere abordar dos dimensiones distintas: la calidad de la información extraída y la precisión del ordenamiento de los candidatos. Para la primera dimensión, enfocada en la estructuración de datos y el Reconocimiento de Entidades Nombradas (NER), se emplean métricas estándar de clasificación. Entre estas destaca el \textit{F-Score} (o F-Measure), que funciona como la media armónica entre la Precisión (porcentaje de entidades extraídas que son correctas) y la Sensibilidad o \textit{Recall} (porcentaje de entidades reales que el sistema logró encontrar). Esta métrica es fundamental para determinar si el sistema es fiable al convertir un PDF en datos estructurados, penalizando tanto la pérdida de información como la generación de datos erróneos \cite{deepai2025}.

Por otro lado, para evaluar el motor de recomendación, es necesario medir la calidad del ranking generado. En este contexto, no basta con identificar candidatos relevantes, sino que estos deben aparecer en las primeras posiciones de la lista. Para ello se utilizan métricas de evaluación \textit{offline} orientadas al ranking, como la Precisión en K (\textit{Precision@K}), que mide la relevancia de los primeros $K$ resultados, y el NDCG (\textit{Normalized Discounted Cumulative Gain}). El NDCG es particularmente valioso en selección de talento porque otorga un peso mayor a los aciertos en la parte superior de la lista, castigando al algoritmo si relega a un candidato ideal a posiciones inferiores, lo cual simula mejor la necesidad real de un reclutador de encontrar al mejor talento rápidamente \cite{pinecone_eval2025}.
\subsection{Vacíos que aborda la presente tesis}

A pesar de los avances en modelos de lenguaje y sistemas de recomendación, la revisión de la literatura evidencia brechas significativas en la aplicación práctica de estas tecnologías para el reclutamiento. En primer lugar, existe una carencia de soluciones que combinen la potencia semántica de los LLMs con la \textbf{explicabilidad} necesaria para la toma de decisiones en Recursos Humanos. La mayoría de los sistemas actuales del estado del arte operan como "cajas negras" que entregan un puntaje de similitud vectorial (embedding matching) sin desglosar las razones detrás de la recomendación, lo que dificulta la validación del criterio por parte de los expertos humanos y disminuye la confianza en la herramienta.

En segundo lugar, los enfoques genéricos de extracción de información suelen fallar al interpretar los matices de dominios especializados como el sector tecnológico. Las herramientas estándar de parsing a menudo no logran inferir habilidades implícitas (por ejemplo, deducir competencia en "Backend" a partir de experiencia en "Java" y "SQL" sin que la palabra esté explícita) o distinguir adecuadamente entre proyectos académicos y experiencia profesional real en perfiles junior, un desafío común identificado en los sistemas basados en palabras clave.

Esta tesis aborda estos vacíos proponiendo una arquitectura que no solo utiliza LLMs para la estructuración robusta de datos no estructurados (superando las limitaciones de formato y parser rígidos), sino que implementa un motor de comparación por aspectos. A diferencia del emparejamiento vectorial monolítico, este enfoque descompone el análisis en dimensiones independientes (experiencia, educación, habilidades) y genera justificaciones en lenguaje natural para cada puntaje, cerrando la brecha entre la precisión algorítmica y la interpretabilidad requerida por los usuarios finales.