\chapter{Estado del Arte}
\label{chap:estado-del-arte}

\section{Sistemas de recomendación para selección de talento}
\subsection{Enfoques clásicos de emparejamiento HV-puesto}

Los enfoques tradicionales de selección de talento se han apoyado fuertemente en los Sistemas de Seguimiento de Candidatos (ATS), los cuales automatizan el filtro inicial de hojas de vida mediante algoritmos de coincidencia de palabras clave (\textit{keyword matching}). Este método consiste fundamentalmente en analizar el texto del currículum en busca de términos específicos extraídos de la descripción del trabajo, tales como habilidades técnicas, títulos de cargo previos o herramientas de software\cite{jobswift2024}.

Para refinar este proceso de búsqueda, estos sistemas suelen implementar lógica booleana que permite a los reclutadores construir consultas complejas para incluir o excluir perfiles según criterios rígidos. El resultado de este procesamiento es habitualmente una puntuación de relevancia o porcentaje de coincidencia, calculado en función de la frecuencia con la que las palabras clave requeridas aparecen en el documento del candidato, generando así un ranking que prioriza los perfiles con mayor densidad de términos coincidentes \cite{jobswift2024}.
\subsection{Limitaciones de las soluciones tradicionales}

A pesar de la automatización que ofrecen los sistemas tradicionales, estos enfrentan barreras técnicas significativas relacionadas con el procesamiento de documentos. Una limitación crítica reside en la incapacidad de muchos ATS antiguos para interpretar correctamente estructuras de diseño complejas. Elementos visuales comunes en hojas de vida modernas, como tablas, cuadros de texto o gráficos, a menudo no son reconocidos por el software de análisis, lo que provoca que la información contenida en ellos sea ignorada o eliminada durante la extracción de datos, resultando en perfiles incompletos \cite{resumemate2025, upskillist2024}.

Un problema técnico frecuente ocurre con el uso de diseños de múltiples columnas. La mayoría de los parsers tradicionales leen el documento de izquierda a derecha y de arriba hacia abajo de manera estrictamente lineal. Esto ocasiona que el texto de una columna se mezcle incorrectamente con el de la columna adyacente, desordenando la información (por ejemplo, mezclando fechas de experiencia con títulos educativos) y haciendo que el contenido sea mas dificil de comprender para el algoritmo de emparejamiento \cite{jobscan2024}.

Estas fallas de interpretación obligan a los candidatos a adaptar sus hojas de vida a formatos planos y poco estéticos para asegurar su legibilidad por la máquina. Cuando el sistema no logra parsear el documento correctamente debido a estos errores de formato, el candidato puede ser descartado automáticamente, independientemente de su idoneidad real para el cargo \cite{upskillist2024}.
\section{Extracción y estructuración de CVs y descripciones de trabajo}
\subsection{Pipelines de NLP para procesamiento de documentos}

El desarrollo de un sistema de análisis de hojas de vida implica la construcción de un flujo de procesamiento secuencial que transforma documentos no estructurados en datos organizados. El primer paso es la conversión del archivo fuente, generalmente una imagen en formato PDF, a texto plano. En esta etapa se utilizan librerías de extracción específicas o tecnologías de OCR cuando el documento es una imagen escaneada, seguidas inmediatamente por una fase de limpieza para eliminar caracteres especiales, espacios redundantes y ruido de formato que pueda interferir con el análisis posterior \cite{geeksforgeeks2024, affinda2025}.

Una vez obtenido el texto limpio, se aplican técnicas de Procesamiento de Lenguaje Natural (NLP) para descomponer y clasificar la información. El proceso inicia con la tokenización y eliminación de palabras vacías (stopwords), preparando el texto para la segmentación. La segmentación divide el documento en bloques lógicos identificables, separando secciones clave como Experiencia Laboral, Educación o Habilidades del resto del contenido, permitiendo que el sistema enfoque su búsqueda en las áreas pertinentes del documento \cite{affinda2025}.

La etapa final y más compleja es la Extracción de Entidades Nombradas (NER). Utilizando modelos de lenguaje pre-entrenados, el sistema recorre los segmentos de texto para identificar y etiquetar datos específicos, clasificándolos en categorías como nombres de personas, organizaciones, fechas o competencias técnicas \cite{geeksforgeeks2024}.
\subsection{Uso de modelos modernos (embeddings y LLMs) para estructuración}

La evolución reciente del Procesamiento de Lenguaje Natural ha introducido paradigmas que superan las limitaciones de las palabras clave. Un avance es el uso de Embeddings, modelos que transforman oraciones o párrafos enteros en vectores numéricos densos dentro de un espacio multidimensional. A diferencia de los métodos tradicionales, estos vectores capturan el significado semántico del texto, permitiendo que el sistema comprenda que términos distintos (como Machine Learning y Aprendizaje Automático) son equivalentes al estar ubicados cerca en el espacio vectorial, lo cual mejora drásticamente la precisión en tareas de búsqueda y emparejamiento \cite{pinecone2025}.

Del mismo modo, la irrupción de los Modelos de Lenguaje Grandes (LLMs), como la serie GPT, ha revolucionado la tarea de extracción de información. A diferencia de los parsers tradicionales que se rompen con diseños complejos, los LLMs poseen una capacidad de comprensión contextual que les permite interpretar hojas de vida con formatos no estándar, tablas o diseños creativos. Estos modelos no se limitan a extraer texto, sino que pueden inferir información implícita y estructurar datos desordenados en formatos rígidos (como JSON) siguiendo instrucciones en lenguaje natural, reduciendo significativamente la necesidad de intervención humana en la limpieza de datos \cite{edenai2024}.


\section{Vacíos que aborda la presente tesis}

A pesar de los avances en modelos de lenguaje y sistemas de recomendación, la revisión de la literatura evidencia brechas significativas en la aplicación práctica de estas tecnologías para el reclutamiento. En primer lugar, existe una carencia de soluciones que combinen la potencia semántica de los LLMs con la explicabilidad necesaria para la toma de decisiones en Recursos Humanos. La mayoría de los sistemas actuales del estado del arte operan como cajas negras que entregan un puntaje de similitud vectorial (embedding matching) sin desglosar las razones detrás de la recomendación, lo que dificulta la validación del criterio por parte de los expertos humanos y disminuye la confianza en la herramienta.

En segundo lugar, los enfoques genéricos de extracción de información suelen fallar al interpretar los matices de dominios especializados como el sector tecnológico. Las herramientas estándar de parsing a menudo no logran inferir habilidades implícitas o distinguir adecuadamente entre proyectos académicos y experiencia profesional real en perfiles junior, un desafío común identificado en los sistemas basados en palabras clave.

Esta tesis aborda estos vacíos proponiendo una arquitectura que no solo utiliza LLMs para la estructuración robusta de datos no estructurados, sino que implementa un motor de comparación por aspectos. A diferencia del emparejamiento vectorial monolítico, este enfoque descompone el análisis en dimensiones independientes y genera justificaciones en lenguaje natural para cada puntaje.