\chapter{Diseño, Arquitectura e Implementación}
\label{chap:diseno-implementacion}

\section{Visión general del sistema}

La aplicación implementada permite que un profesional de recursos humanos cargue hojas de vida en PDF y descripciones de trabajo en texto, las estructure automáticamente mediante modelos de lenguaje y obtenga, en cuestión de segundos, un score de compatibilidad por candidato acompañado de un desglose explicativo por aspectos clave (experiencia, habilidades, educación, entre otros). Detrás de esta interacción sencilla se articula una arquitectura en capas que separa la interfaz de usuario, la lógica de negocio, el motor de recomendación y la persistencia de datos, lo que facilita la trazabilidad de cada decisión y la evolución independiente de los componentes.

Este capítulo describe cómo se materializó la metodología propuesta, desde el diseño de la arquitectura del sistema hasta los aspectos centrales de su implementación. Se parte de una vista global que muestra cómo el usuario interactúa con la aplicación, cómo el frontend se comunica con la API y cómo esta, a su vez, orquesta servicios, núcleo de recomendación y acceso a datos. A continuación se muestra la figura que resume esta arquitectura por capas y servirá como referencia visual a lo largo del capítulo.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Arquitectura_aplicacion.png}
    \caption{Arquitectura de la aplicación}
    \label{fig:diagram-arquitectura}
\end{figure}

\subsection{Arquitectura general y flujo de usuario}

La arquitectura propuesta sigue un patrón cliente–servidor en el que el usuario de recursos humanos interactúa con un frontend web desarrollado en React. Este frontend consume una API REST construida con FastAPI, encargada de recibir las solicitudes (subir HVs, registrar descripciones de trabajo, lanzar análisis y consultar resultados) y delegar la lógica de negocio a una capa de servicios. Los servicios coordinan dos elementos principales del backend: por un lado, el \emph{core} del sistema, donde se ubican la limpieza de texto, la estructuración basada en modelos de lenguaje (LLM) y el motor de recomendación por aspectos; por otro lado, la capa de datos, compuesta por repositorios y una base de datos SQLite que almacena HVs, descripciones y análisis. De esta forma, el flujo típico es: usuario \(\rightarrow\) frontend \(\rightarrow\) API \(\rightarrow\) servicios \(\rightarrow\) core y datos, y de regreso hacia el usuario con un score final y explicaciones asociadas.

La Figura \ref{fig:diagram-arquitectura} se organiza en cuatro componentes principales. En primer lugar, el \textbf{frontend}, que ofrece la interfaz donde se cargan documentos y se consultan análisis. En el backend, se distinguen tres bloques: (i) la \textbf{capa de aplicación}, que agrupa API y servicios y define los flujos de casos de uso; (ii) la \textbf{capa de datos}, compuesta por repositorios y la base de datos responsable de la persistencia; y (iii) el \textbf{core} de la aplicación, donde residen el pipeline de NLP (limpieza y estructuración) y el motor de recomendación con sus comparadores por aspecto. Esta separación permite razonar sobre el sistema tanto desde la perspectiva de arquitectura de software como desde la perspectiva de flujo de información.

\subsection{Estructura del capítulo}

Para facilitar la lectura, el resto del capítulo se organiza siguiendo la descomposición que sugiere la arquitectura. Primero se presenta en detalle el backend por capas, comenzando por la API y los servicios, continuando con los repositorios y el modelo de datos, despues se profundiza en el core del sistema conectando estos elementos con las decisiones metodológicas descritas en el capítulo anterior. Finalmente, se describe el rol del frontend y su interacción con la API, resaltando cómo se materializa el flujo de usuario en la interfaz y cómo se presentan los resultados de manera que apoyen la toma de decisiones en el primer filtro de selección.

\section{Vista general del backend}

El backend del sistema implementa la lógica de negocio necesaria para transformar documentos en estructuras comparables, ejecutar el motor de recomendación y almacenar de forma persistente los resultados. Está organizado en capas bien definidas que separan el API y la logica de negocio (servicios), el núcleo de procesamiento (limpieza, estructuración y comparación) y el acceso a datos (repositorios y base de datos). Esta separación favorece la mantenibilidad y permite razonar sobre cada responsabilidad de forma aislada, al tiempo que facilita la reutilización de componentes en distintos flujos de la aplicación.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Arquitectura_aplicacion_backend.png}
    \caption{Arquitectura por capas del backend}
    \label{fig:arquitectura_backend}
\end{figure}
Desde el punto de vista de la comunicación interna, la API recibe las solicitudes provenientes del frontend y delega en la capa de servicios tareas como procesar un nuevo CV, registrar una oferta o ejecutar un análisis. Los servicios, a su vez, invocan el core para limpiar y estructurar texto, o para calcular puntajes de compatibilidad, y utilizan los repositorios para leer y escribir en la base de datos SQLite. Como se aprecia en la Figura \ref{fig:arquitectura_backend}, las llamadas fluyen de API a servicios, de servicios al core, y de allí a los repositorios, cerrando el ciclo al devolver al usuario información enriquecida (scores, breakdowns y resúmenes) a través de la misma API.

\section{Capa de aplicación: API y servicios}

\subsection{Introducción a la capa de aplicación}
La capa de aplicación es el punto de entrada del backend: recibe las peticiones HTTP provenientes del frontend, las valida y las traduce en operaciones de negocio concretas. En ella se ubican la API, implementada con FastAPI, y la capa de servicios, responsable de coordinar el pipeline de limpieza, estructuración, recomendación y acceso a datos. De esta forma, la lógica de negocio queda encapsulada en servicios reutilizables, mientras que la API se limita a exponer endpoints bien definidos y a gestionar aspectos transversales como serialización, manejo de errores y documentación. En la siguiente Figura \ref{fig:capa_de_aplicacion_en_backend} se muestra cual es la capa de aplicación en el backend.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Arquitectura_backend_capa_de_aplicacion.png}
    \caption{Capa de aplicación en el backend}
    \label{fig:capa_de_aplicacion_en_backend}
\end{figure}


En términos de flujo, como se puede observar en la siguiente Figura \ref{fig:Capa_de_aplicacion} , el frontend invoca los endpoints de la API para manejar todo lo referente a las HV, descripciones o análisis; la API delega exclusivamente en el servicio correspondiente (CVService, JobService, RecommendationService o AnalysisService), el cual encapsula toda la lógica de orquestación. Es el servicio quien invoca al core para el procesamiento y quien interactúa con los repositorios para la persistencia de datos. Las figuras de secuencia incluidas en el apéndice de figuras (Figuras \ref{fig:seq-subir-cv}, \ref{fig:seq-crear-job} y \ref{fig:seq-analizar}) ilustran estos flujos paso a paso para los casos de uso principales.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Capa_de_aplicacion.png}
    \caption{Arquitectura de la capa de aplicación}
    \label{fig:Capa_de_aplicacion}
\end{figure}
\subsection{API REST: endpoints y tecnologías}

La API se implementó utilizando FastAPI sobre Python 3.11, aprovechando su soporte para tipado estático y generación automática de documentación. El servidor de desarrollo se levanta con Uvicorn, exponiendo la aplicación en local, mientras que la configuración de CORS permite el acceso controlado desde el cliente web. Adicionalmente, la documentación interactiva detallada de todos los endpoints, incluyendo esquemas de entrada y salida, se genera automáticamente y puede consultarse en tiempo real accediendo a la ruta \texttt{/docs} del servidor (por defecto \texttt{http://localhost:8000/docs}).

Para estructurar la comunicación, los endpoints se agrupan por recurso funcional como se ilustra en la Figura \ref{fig:Figura_API}. Esta organización facilita la mantenibilidad y permite que cada controlador se especialice en un tipo de entidad. A continuación, se describen los grupos principales de endpoints mostrados en el diagrama, cuyo detalle técnico completo (parámetros y respuestas) se encuentra documentado en el Apéndice \ref{app:api-docs}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Figura_API.png}
    \caption{Diseño de la API}
    \label{fig:Figura_API}
\end{figure}

\subsubsection*{Gestión de Hojas de Vida (CVs)}
El componente \texttt{CV\_Endpoint} agrupa las operaciones para la administración del ciclo de vida de las hojas de vida. Su función principal es la carga de documentos (\texttt{crear\_cv}), donde el sistema recibe un archivo PDF, extrae su contenido y lo procesa. Adicionalmente, expone métodos para listar todos los candidatos (\texttt{listar\_cvs}), consultar el detalle de uno específico (\texttt{obtener\_cv}), buscar por nombre (\texttt{buscar\_cvs}) y eliminar registros (\texttt{eliminar\_cv}). Puede consultar el detalle técnico de estos endpoints en el Apéndice \ref{sec:api-hvs}.

\subsubsection*{Gestión de Descripciones de Trabajo (Jobs)}
El componente \texttt{Job\_Endpoint} gestiona las ofertas laborales de manera análoga. Permite registrar nuevas descripciones mediante texto plano (\texttt{crear\_job}), las cuales son procesadas para estructurar requisitos. Las operaciones de consulta (\texttt{listar\_jobs}, \texttt{obtener\_job}, \texttt{buscar\_jobs}) facilitan la recuperación de información, mientras que \texttt{eliminar\_job} permite la gestión del ciclo de vida de la vacante. Puede consultar el detalle técnico de estos endpoints en el Apéndice \ref{sec:api-jobs}.

\subsubsection*{Análisis y Estadísticas}
El componente \texttt{Analysis\_Endpoint} actúa como el núcleo funcional donde converge la información. Su método principal, \texttt{analizar}, desencadena la comparación entre una HV y un Job. Este módulo también ofrece endpoints para consultar el historial (\texttt{listar\_analyses}), obtener detalles específicos (\texttt{obtener\_analysis}), generar rankings de mejores candidatos (\texttt{analyses\_por\_job} o top candidatos) y visualizar métricas generales del sistema (\texttt{estadisticas}). Puede consultar el detalle técnico de estos endpoints en el Apéndice \ref{sec:api-analysis}.

Cada endpoint está diseñado bajo el patrón de \emph{application services}, delegando la totalidad del procesamiento y la persistencia a la capa de servicios. Su responsabilidad se limita a validar la estructura de la petición HTTP, invocar el método de servicio adecuado y transformar la respuesta de negocio en una respuesta HTTP con los códigos de estado estándar.

\subsection{Servicios y lógica de negocio}

La capa de servicios sigue un patrón de \emph{application services} que centraliza la lógica de negocio, orquestando la interacción entre los componentes del sistema. Como se ilustra en la Figura \ref{fig:FiguraServices}, estos servicios actúan como intermediarios que coordinan el acceso a datos y la ejecución de procesos del núcleo, garantizando que la API permanezca desacoplada de la implementación subyacente.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Figura_Services.png}
    \caption{Componentes de la capa de servicios}
    \label{fig:FiguraServices}
\end{figure}

El backend se estructura en cuatro servicios principales, cada uno con responsabilidades delimitadas:

\begin{itemize}
  \item \textbf{CVService}: Gestiona el ciclo de vida de las hojas de vida. Sus funciones principales incluyen procesar archivos PDF (\texttt{process\_cv\_from\_file}) interactuando con el nucleo y administrar la persistencia de datos (creación, búsqueda y eliminación) interactuando directamente con la capa de repositorios.
  
  \item \textbf{JobService}: Centraliza la lógica relacionada con las descripciones de trabajo. Coordina la estructuración de texto plano mediante IA (\texttt{process\_job\_from\_text}) interactuando con el core y gestiona todas las operaciones CRUD sobre las ofertas laborales, asegurando que los datos almacenados cumplan con el esquema requerido.

  \item \textbf{AnalysisService}: Administra el historial y consulta de resultados. Este servicio se encarga de persistir los análisis generados, recuperar detalles específicos, y ofrecer consultas agregadas como rankings de candidatos  o estadísticas globales del sistema, sirviendo como fuente de verdad para la visualización de datos.

  \item \textbf{RecommendationService}: Encapsula la inteligencia del sistema. Su método principal (\texttt{analyze}) orquesta el motor de recomendación (\texttt{RecommendationEngine}) para comparar los datos estructurados de una HV y un Job. Este servicio es responsable de calcular los puntajes de compatibilidad y generar los desgloses explicativos, delegando la persistencia del resultado final al \texttt{AnalysisService}.
\end{itemize}

Este diseño modular facilita la mantenibilidad y las pruebas unitarias, ya que cada servicio puede aislarse para verificar su lógica de orquestación independientemente de la interfaz HTTP o de la base de datos.
\section{Capa de Datos y Persistencia}
\label{sec:capa-datos}

\subsection{Introducción a la capa de datos}
Esta sección detalla la capa encargada de garantizar la persistencia y consistencia de la información dentro del sistema. Como se aprecia en la Figura \ref{fig:capa_de_datos_en_backend}, donde se resalta en amarillo el componente de datos, esta capa fundamenta el almacenamiento del sistema. La arquitectura se basa en el patrón \textit{Repository}, el cual actúa como una colección en memoria para objetos del dominio, abstrayendo los detalles de implementación de la base de datos subyacente. En este caso, se emplea SQLite como motor de almacenamiento debido a su ligereza y suficiencia para el alcance del prototipo, gestionado a través de \texttt{SQLAlchemy} como ORM (Object-Relational Mapper). Esta combinación permite manipular los datos utilizando clases y objetos de Python, facilitando la evolución del esquema sin atar la lógica de negocio a consultas SQL específicas.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Arquitectura_backend_capa_de_datos.png}
    \caption{Capa de datos en el backend}
    \label{fig:capa_de_datos_en_backend}
\end{figure}

\subsection{Implementación de Repositorios y Flujo de Persistencia}
\subsection{Modelo de Datos e Implementación de Persistencia}

El diseño de la capa de datos se fundamenta en un modelo relacional simple pero robusto, representado en la Figura \ref{fig:er-modelo}, que permite almacenar de forma estructurada la información clave del proceso de selección. El esquema consta de tres entidades principales: \textbf{CVS}, que almacena los perfiles de los candidatos; \textbf{JOBS}, que contiene las descripciones de las vacantes; y \textbf{ANALYSES}, una tabla intermedia que materializa la relación muchos a muchos entre candidatos y vacantes, registrando los resultados detallados de cada evaluación de compatibilidad.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{ModeloEntidadRelacion.png}
    \caption{Modelo de datos (ER)}
    \label{fig:er-modelo}
\end{figure}

Para llevar este modelo a la implementación, se emplea un diseño que desacopla la definición de las entidades de su lógica de manipulación, tal como se introdujo en la Figura \ref{fig:capa_de_datos_en_backend}. Como se detalla en el Apéndice (Figura \ref{fig:DetallesImplementacionDatos}), se utilizan clases de modelo (\textit{Models}) que heredan de la base declarativa de SQLAlchemy para mapear los atributos definidos en el diagrama ER a columnas físicas de la base de datos SQLite.

La manipulación de estos datos se delega a los repositorios, que actúan como gestores de transacciones. El flujo de persistencia, ilustrado en la Figura \ref{fig:flujo_persistencia_analysis}, sigue una secuencia rigurosa para asegurar la integridad referencial: cuando un servicio solicita guardar un análisis, el repositorio transforma los datos en una instancia del modelo ORM, gestiona la sesión de la base de datos y ejecuta la confirmación (\textit{commit}), retornando finalmente el objeto con su identificador persistido para su uso en las capas superiores.

\section{Core del sistema}
\subsection{Introducción al core de la aplicación}
El \textit{Core} del sistema representa el componente central donde reside la inteligencia de la aplicación, separado lógica y funcionalmente de las capas de servicio y persistencia. Como se aprecia en la Figura \ref{fig:arquitectura_backe_core}, este núcleo agrupa los módulos de procesamiento intensivo en dos grandes motores: el motor de estructuración, encargado de interpretar y estandarizar la información no estructurada de documentos y textos; y el motor de recomendación, que orquesta los algoritmos de comparación para evaluar la compatibilidad entre candidatos y vacantes. Esta disposición permite que el procesamiento cognitivo y el cálculo de similitudes evolucionen de forma independiente a la lógica de control de la API.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Arquitectura_backend_core.png}
    \caption{Core de la aplicación en el backend}
    \label{fig:arquitectura_backe_core}
\end{figure}

\section{Motor de Estructuración }

El componente de estructuración es responsable de transformar documentos no estructurados —como hojas de vida en formato PDF y descripciones de trabajo en texto libre— en objetos de datos estandarizados (JSON) que el sistema pueda procesar algorítmicamente. Este módulo actúa como puente entre la entrada de usuario y el motor de recomendación, garantizando que la información heterogénea sea normalizada bajo un esquema común.

\subsection{Arquitectura del componente de estructuración}

Como se ilustra en la Figura \ref{fig:arquitectura_estructuracion}, el flujo de información comienza en la capa de servicios, donde `CVService` y `JobService` actúan como orquestadores. Dependiendo del tipo de entrada, la solicitud se enruta hacia uno de los dos pipelines especializados: el `Pipeline HV` para el procesamiento de documentos PDF, o el `Pipeline Descripción` para textos de ofertas laborales. Ambos pipelines convergen en el módulo central de estructuración, que encapsula la lógica de interacción con los modelos de lenguaje.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{DiagramaEstructuraciones.png}
    \caption{Arquitectura del componente de estructuración}
    \label{fig:arquitectura_estructuracion}
\end{figure}

El diseño desacopla la recepción del archivo de su procesamiento cognitivo. Mientras que los servicios gestionan la carga y persistencia preliminar, el núcleo de estructuración se encarga exclusivamente de la extracción de entidades, validación de esquemas y limpieza de datos. Esto permite que, aunque las fuentes de datos sean distintas (.pdf vs .txt), el resultado final sea siempre una estructura coherente y lista para ser consumida por los algoritmos de comparación.

\section{Pipeline de procesamiento de hojas de vida}

\label{subsec:pipeline-hv}

El procesamiento de hojas de vida constituye uno de los flujos críticos del sistema, dado que transforma documentos PDF no estructurados en datos procesables. Como se detalla en la Figura \ref{fig:pipeline_hv}, este proceso se orquesta secuencialmente a través de tres fases principales: extracción, estructuración cognitiva y validación.

\begin{samepage}
\noindent
\begin{minipage}[t]{0.62\textwidth}
\vspace{0pt}
\indent El flujo inicia con la recepción del archivo PDF, el cual ingresa a la \textbf{Fase 1: Extracción y Limpieza}. En esta etapa, se utiliza la librería PyMuPDF para extraer el contenido textual crudo, el cual es sometido inmediatamente a un proceso de normalización (detallado en la Sección \ref{subsec:fase-limpieza}) para eliminar ruido y caracteres no deseados, resultando en un texto limpio apto para el procesamiento.

\medskip

\indent Posteriormente, en la \textbf{Fase 2: Estructuración de Elementos}, el sistema emplea una estrategia iterativa de segmentación semántica. En lugar de procesar el documento en un único paso, se construyen prompts específicos para cada sección clave (información personal, experiencia, educación, entre otros) y se invocan modelos de lenguaje (GPT-4o-mini) para extraer cada componente por separado. Esta estrategia, que profundizaremos en la Sección \ref{subsec:fase-estructuracion-ia}, maximiza la precisión de la extracción al reducir la ventana de contexto necesaria para cada consulta.

\medskip

\indent Finalmente, la \textbf{Fase 3: Consolidación y Validación} se encarga de ensamblar las respuestas parciales. Las salidas del modelo son depuradas de marcas de formato (markdown), unificadas en un único objeto JSON y sometidas a una validación estricta de esquema para garantizar la integridad de los datos, proceso que se describe en la Sección \ref{subsec:fase-validacion}. El resultado es un JSON estructurado final, listo para ser almacenado y utilizado por el motor de recomendación.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.35\textwidth}
    \vspace{0pt}
    \centering
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.95\textwidth, height=0.65\textheight, keepaspectratio]{Pipeline_Estructuracion_CV.png}
        \caption{Pipeline de estructuración de hojas de vida}
        \label{fig:pipeline_hv}
    \end{figure}
\end{minipage}
\end{samepage}
\subsection{Fase 1: Extracción y limpieza de texto}
\label{subsec:fase-limpieza}

La primera fase del pipeline tiene como objetivo transformar el archivo PDF binario en una cadena de texto normalizada, libre de ruido que pueda interferir con el procesamiento del modelo de lenguaje. Como se observa en la Figura \ref{fig:limpieza_cv}, este proceso se divide en dos etapas: la extracción inicial mediante herramientas de bajo nivel y un pipeline de limpieza sintáctica.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Fase_Limpieza_Pipeline_CV.png}
    \caption{Proceso de limpieza y normalización de texto}
    \label{fig:limpieza_cv}
\end{figure}

Inicialmente, el sistema utiliza \textit{PyMuPDF} para extraer el contenido textual de todas las páginas del documento, concatenándolas en una única cadena de ("Texto Crudo") como se ve en la figura \ref{. Este texto suele contener irregularidades derivadas del formato PDF, como saltos de línea arbitrarios, caracteres de control o problemas de codificación. Para mitigar esto, el texto pasa por una serie de funciones de limpieza secuenciales implementadas en el módulo `limpieza.py`:

\begin{enumerate}
    \item \textbf{Normalización de caracteres (Quitar Tildes):} Se convierten caracteres acentuados a sus equivalentes ASCII (ej: 'á' $\rightarrow$ 'a') para estandarizar la entrada, reduciendo la variabilidad léxica sin perder el significado semántico esencial.
    \item \textbf{Filtrado de ruido (Quitar Símbolos Raros):} Se eliminan caracteres no alfanuméricos o símbolos de formato problemáticos (como §, ï, o bullets personalizados) que suelen aparecer al extraer texto de documentos con diseño complejo.
    \item \textbf{Reconstrucción léxica (Unir Letras Separadas):} Se aplica una heurística basada en expresiones regulares para detectar y corregir palabras que aparecen con espaciado inter-carácter (ej: "T I T U L O" $\rightarrow$ "TITULO"), un artefacto común en encabezados de hojas de vida.
    \item \textbf{Limpieza final:} Se eliminan caracteres especiales restantes y puntuación excesiva, y se convierte todo el texto a minúsculas para garantizar consistencia en el procesamiento posterior.
\end{enumerate}

A modo de ilustración, el siguiente ejemplo muestra la transformación que sufre un fragmento de texto típico tras pasar por este pipeline:

\begin{quotation}
    \small
    \noindent \textbf{Entrada (Texto Crudo):} "D E S A R R O L L A D O R -- FULL STACK. ¡Experiencia en: Python, SQL & Java! (2023)." \\
    \noindent \textbf{Salida (Texto Limpio):} "desarrollador full stack experiencia en python sql java 2023"
\end{quotation}

El resultado ("Texto Limpio") en la Figura \ref{fig:limpieza_cv} es una representación simplificada y uniforme del contenido del CV, optimizada para ser tokenizada eficientemente por el modelo de IA en la siguiente fase.

\subsection{Fase 2: Estrategia de prompting y estructuración}
\label{subsec:fase-estructuracion-ia}

Una vez obtenido el texto limpio, el sistema inicia la fase de estructuración cognitiva. A diferencia de enfoques tradicionales que intentan extraer toda la información en una sola consulta (lo cual suele saturar la ventana de contexto del modelo y generar alucinaciones), esta implementación utiliza una estrategia secuencial y modular basada en la segmentación por tópicos.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\linewidth]{Fase_Estructuracion_Pipeline_CV.png}
    \caption{Proceso iterativo de estructuración con LLMs}
    \label{fig:estructuracion_iterativa}
\end{figure}

Como se ilustra en la Figura \ref{fig:estructuracion_iterativa}, el texto completo del CV se inyecta repetidamente en un ciclo de consultas, donde en cada iteración se utiliza un \textit{prompt} específico diseñado para extraer una única categoría de información (Información Personal, Educación, Experiencia, Habilidades, etc.). Esta metodología combina tres estrategias de \textit{Prompt Engineering} definidas en el marco teórico:

\begin{itemize}
    \item \textbf{Role Prompting:} Se instruye al modelo explícitamente: \textit{``Eres un experto en extraer información estructurada de hojas de vida''}. Esta técnica, identificada como el patrón de Persona \cite{promptguide2024}, condiciona la atención del modelo a entidades y terminología relevantes del dominio de reclutamiento.
    \item \textbf{Structured Output Prompting:} Se define un esquema JSON rígido dentro del prompt y se prohíbe la generación de texto conversacional adicional. Esta restricción es fundamental para garantizar la interoperabilidad con el sistema backend \cite{promptguide2024}.
    \item \textbf{Zero-Shot Prompting:} No se proveen ejemplos de CVs previos en el contexto; el modelo opera basándose únicamente en las instrucciones semánticas y el esquema objetivo, aprovechando su capacidad de generalización pre-entrenada \cite{promptguide2024}.
\end{itemize}

A continuación se presenta un ejemplo de la estructura de prompt utilizada para la extracción de información personal:

\begin{quotation}
    \small
        \noindent \textbf{System Message:} ``Eres un experto en extraer información estructurada de hojas de vida. IMPORTANTE: Responde ÚNICAMENTE con la estructura que se te pide, sin texto adicional.'' \\
        \noindent \textbf{User Message:} ``Extrae únicamente la información personal básica del siguiente CV. Responde en formato JSON con esta estructura exacta:'' 


    \begin{verbatim}
    {
        "name": "nombre completo",
        "email": "correo electrónico",
        "phone": "número de teléfono",
        "location": "ubicación/ciudad"
    }
    \end{verbatim}
    Si no encuentras alguna información, deja el campo como DESCONOCIDO"
\end{quotation}

Este enfoque granular permite validar y corregir cada sección de forma independiente (ver Apéndice \ref{tab:prompts-cv} para el listado completo de prompts utilizados). Si el modelo falla al extraer la experiencia, no afecta la extracción de la educación, lo que aumenta significativamente la robustez del sistema frente a documentos con formatos atípicos.
\subsection{Fase 3: Consolidación y validación de datos}
\label{subsec:fase-validacion}

La etapa final del pipeline asegura que la información generada por el modelo sea sintácticamente válida y estructuralmente coherente antes de ser almacenada. Como se muestra en la Figura \ref{fig:validacion_json}, este proceso aborda uno de los desafíos más comunes al trabajar con LLMs: la inestabilidad en el formato de salida.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Fase_Validacion_Pipeline_Descripcion.png}
    \caption{Flujo de consolidación y validación de datos}
    \label{fig:validacion_json}
\end{figure}

El flujo implementado ejecuta las siguientes operaciones:

\begin{enumerate}
    \item \textbf{Limpieza de Markdown:} Los modelos de chat tienden a envolver el código en bloques de formato (ej: ```json ... ```). El sistema detecta y elimina estos delimitadores para extraer el objeto JSON puro.
    \item \textbf{Parsing Seguro:} Se utiliza un decodificador JSON robusto que captura errores de sintaxis. Si el modelo genera un JSON malformado, el sistema captura la excepción y retorna una estructura vacía por defecto en lugar de romper la ejecución del servicio.
    \item \textbf{Unificación de Fragmentos:} Los objetos JSON parciales obtenidos en la Fase 2 (uno por cada sección: personal, educación, etc.) se fusionan en un único diccionario maestro que guarda la informacion de todos los elementos.
    \item \textbf{Validación de Schema (Inyección de Defaults):} Se verifica que todas las claves obligatorias existan. Si el modelo omitió algún campo (por ejemplo, no encontró certificaciones), el sistema inyecta automáticamente un valor por defecto (lista vacía o cadena vacía), garantizando que el objeto final cumpla estrictamente con la estructura esperada por el motor de recomendación.
\subsection{Pipeline de procesamiento de descripciones de trabajo}
\label{subsec:pipeline-jobs}



\begin{samepage}
\noindent
\begin{minipage}[t]{0.62\textwidth}
\vspace{0pt}
\indent De manera análoga al procesamiento de CVs, el sistema implementa un flujo especializado para transformar las descripciones de vacantes en estructuras de datos normalizadas. Como se observa en la Figura \ref{fig:pipeline_job}, este pipeline comparte la arquitectura secuencial de tres fases, adaptando sus componentes internos a la naturaleza textual y semántica de las ofertas laborales.
\medskip

\indent El proceso comienza con la recepción del texto de la oferta laboral. En la \textbf{Fase 1}, se realiza una limpieza preliminar similar a la descrita anteriormente, aunque simplificada, dado que la entrada suele ser texto plano en lugar de un documento binario complejo.

\medskip

\indent La \textbf{Fase 2} constituye el núcleo diferencial de este pipeline. Aquí, la estrategia de segmentación se ajusta para identificar secciones propias de una vacante, como responsabilidades, beneficios y información basica. Además, se incorporan capacidades de inferencia deductiva (p.ej., deducir habilidades blandas a partir de las funciones del cargo), un aspecto que se detallará en la Sección \ref{subsec:fase-estructuracion-job}.

\medskip

\indent Finalmente, la \textbf{Fase 3} replica el mecanismo de consolidación y validación, asegurando que el objeto JSON resultante cumpla con el esquema requerido para la comparación bidireccional con los candidatos.
\end{minipage}%
\hfill
\begin{minipage}[t]{0.22\textwidth}
    \vspace{0pt}
    \centering
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.95\textwidth, height=0.65\textheight, keepaspectratio]{Pipeline_Estructuracion_Descripcion.png}
        \caption{Pipeline de estructuración de descripciones de trabajo}
        \label{fig:pipeline_job}
    \end{figure}
\end{minipage}
\end{samepage}

\subsubsection{Fase 1: Limpieza y normalización}
Aunque la entrada es texto plano y no requiere extracción óptica o binaria compleja (como PyMuPDF), se aplica el mismo módulo de limpieza sintáctica descrito en la Sección \ref{subsec:fase-limpieza}. Esto garantiza que caracteres especiales, problemas de codificación o espaciados irregulares sean corregidos antes de invocar al modelo de lenguaje, manteniendo la consistencia en la calidad de los datos de entrada.

\subsubsection{Fase 2: Estrategia de prompting y estructuración}
\label{subsec:fase-estructuracion-job}

En esta fase se materializa la inteligencia del sistema para comprender los requisitos de una vacante. Siguiendo el patrón de segmentación semántica utilizado en el pipeline de CVs, el texto de la descripción se procesa iterativamente utilizando prompts especializados para extraer componentes como información básica, responsabilidades, educación y beneficios (Figura \ref{fig:estructuracion_job}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\linewidth]{Fase_Estructuracion_Pipeline_Descripcion.png}
    \caption{Estrategia de estructuración de ofertas laborales}
    \label{fig:estructuracion_job}
\end{figure}

Sin embargo, este pipeline introduce dos capacidades cognitivas adicionales que lo diferencian de la extracción literal:

\begin{itemize}
    \item \textbf{Deducción de Habilidades Blandas (ver Figura \ref{fig:Flujo_Inferir_Habilidades_Blandas}):} A menudo, las ofertas laborales no listan explícitamente las habilidades blandas requeridas, pero estas se encuentran implícitas en las responsabilidades (p.\,ej., ``liderar equipos ágiles'' implica ``liderazgo'' y ``comunicación''). El sistema primero intenta extraer habilidades blandas explícitas mediante el prompt correspondiente. En caso de que no se encuentren, se activa automáticamente un mecanismo de inferencia que analiza semánticamente las funciones del cargo mediante las responsabilidades extraías por el LLM y deduce las 3-5 habilidades blandas más críticas, enriqueciendo el perfil de la vacante más allá del texto literal. Este flujo se puede observar en el diagrama de flujos mostrado en el indice en la Figura \ref{fig:Flujo_Inferir_Habilidades_Blandas}.
    
    \item \textbf{Detección de Idioma Contextual (ver Figura \ref{fig:Flujo_Inferir_Lenguaje}):} El sistema prioriza la extracción de requisitos de idioma explícitos mediante el prompt de idiomas estándar. Sin embargo, si la oferta no especifica idiomas (retorna lista vacía), se activa un mecanismo de recuperación que emplea un prompt de clasificación zero-shot para identificar el idioma en el que está redactado el texto completo de la oferta. Si se detecta, por ejemplo, que la oferta está en inglés, el sistema infiere automáticamente un requisito de ``Inglés - Nivel Intermedio'', asegurando que el motor de recomendación filtre candidatos aptos lingüísticamente incluso ante descripciones incompletas. Este flujo se puede observar en el diagrama de flujos mostrado en el indice en la Figura \ref{fig:Flujo_Inferir_Lenguaje}.
\end{itemize}

Estas estrategias transforman el proceso de una simple extracción de texto a una comprensión semántica de la necesidad del empleador, generando un perfil de vacante más robusto para el emparejamiento posterior. Los prompts específicos utilizados para estas inferencias se detallan en el Apéndice \ref{tab:prompts-job}, filas ``Inferencia soft skills'' y ``Detección de idioma''.

\subsubsection{Fase 3: Consolidación y validación}
Esta fase es funcionalmente idéntica a la descrita en la Sección \ref{subsec:fase-validacion}. Se ejecutan los mismos procesos de limpieza de markdown, unificación de fragmentos JSON y validación de esquema con inyección de valores por defecto. La única diferencia radica en el esquema objetivo, que en este caso corresponde a la estructura de una oferta laboral (incluyendo campos como rango salarial, modalidad de trabajo y beneficios).




